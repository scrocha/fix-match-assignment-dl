{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import RandAugment\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from typing import List, Tuple, Callable\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES: int = 10\n",
    "BATCH_SIZE: int = 64\n",
    "MU: int = 7\n",
    "LR: float = 0.03\n",
    "MOMENTUM: float = 0.9\n",
    "WEIGHT_DECAY: float = 5e-4\n",
    "\n",
    "LAMBDA_U: float = 1.0\n",
    "T: float = 0.95\n",
    "\n",
    "SEED: int = 42\n",
    "DATA_DIR: str = './data'\n",
    "SAVE_DIR: str = './experiments'\n",
    "\n",
    "SAVE_N_EPOCHS: int = 20\n",
    "TOTAL_EPOCHS: int = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef52a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10SemiSupervised(Dataset):\n",
    "    def __init__(self, base_dataset: Dataset, transform: Callable):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        img, label = self.base_dataset[idx]\n",
    "        return self.transform(img), label\n",
    "\n",
    "\n",
    "class TwoCropsTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_transform: Callable,\n",
    "        weak_post_transform: Callable,\n",
    "        strong_post_transform: Callable,\n",
    "    ):\n",
    "        self.base_transform = base_transform\n",
    "        self.weak_post_transform = weak_post_transform\n",
    "        self.strong_post_transform = strong_post_transform\n",
    "\n",
    "    def __call__(self, x: Image.Image) -> List[torch.Tensor]:\n",
    "        base_img = self.base_transform(x)\n",
    "\n",
    "        img_w = self.weak_post_transform(base_img)\n",
    "        img_s = self.strong_post_transform(base_img)\n",
    "\n",
    "        return [img_w, img_s]\n",
    "\n",
    "\n",
    "def get_dataloaders(num_labeled_per_class: int):\n",
    "    cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "    cifar10_std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "    base_spatial_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    weak_post_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    strong_post_transform = transforms.Compose(\n",
    "        [\n",
    "            RandAugment(num_ops=2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "            # CORREÇÃO 1: Simular Cutout (quadrado) com ratio=(1.0, 1.0) e 1/5 da imagem\n",
    "            transforms.RandomErasing(\n",
    "                p=0.5, scale=(0.2, 0.2), ratio=(1.0, 1.0), value=0\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    labeled_transform = transforms.Compose(\n",
    "        [base_spatial_transform, weak_post_transform]\n",
    "    )\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_data = datasets.CIFAR10(\n",
    "        DATA_DIR, train=True, download=True, transform=None\n",
    "    )\n",
    "\n",
    "    test_data = datasets.CIFAR10(\n",
    "        DATA_DIR, train=False, download=True, transform=test_transform\n",
    "    )\n",
    "\n",
    "    targets = np.array(train_data.targets)\n",
    "    labeled_indices = []\n",
    "    unlabeled_indices = []\n",
    "\n",
    "    for i in range(NUM_CLASSES):\n",
    "        indices = np.where(targets == i)[0]\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        labeled_indices.extend(indices[:num_labeled_per_class])\n",
    "        unlabeled_indices.extend(indices[num_labeled_per_class:])\n",
    "\n",
    "    random.shuffle(labeled_indices)\n",
    "    random.shuffle(unlabeled_indices)\n",
    "\n",
    "    print(f\"Total de amostras: {len(targets)}\")\n",
    "    print(f\"Amostras rotuladas: {len(labeled_indices)}\")\n",
    "    print(f\"Amostras não rotuladas: {len(unlabeled_indices)}\")\n",
    "\n",
    "    labeled_dataset = CIFAR10SemiSupervised(\n",
    "        Subset(train_data, labeled_indices), transform=labeled_transform\n",
    "    )\n",
    "\n",
    "    unlabeled_dataset = CIFAR10SemiSupervised(\n",
    "        Subset(train_data, unlabeled_indices),\n",
    "        transform=TwoCropsTransform(\n",
    "            base_spatial_transform, weak_post_transform, strong_post_transform\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    labeled_batch_size = min(BATCH_SIZE, len(labeled_indices))\n",
    "    if labeled_batch_size == 0:\n",
    "        labeled_batch_size = 1\n",
    "\n",
    "    unlabeled_batch_size = BATCH_SIZE * MU\n",
    "\n",
    "    labeled_loader = DataLoader(\n",
    "        labeled_dataset,\n",
    "        batch_size=labeled_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    unlabeled_loader = DataLoader(\n",
    "        unlabeled_dataset,\n",
    "        batch_size=unlabeled_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return labeled_loader, unlabeled_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be90943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18_for_cifar(num_classes: int = NUM_CLASSES) -> nn.Module:\n",
    "    model = models.resnet18(weights=None, num_classes=num_classes)\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "    )\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_resnet50_for_cifar(num_classes: int = NUM_CLASSES) -> nn.Module:\n",
    "    model = models.resnet50(weights=None, num_classes=num_classes)\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "    )\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "\n",
    "def fixmatch_loss(\n",
    "    logits_x: torch.Tensor,\n",
    "    targets_x: torch.Tensor,\n",
    "    logits_u_w: torch.Tensor,\n",
    "    logits_u_s: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    loss_x = nn.CrossEntropyLoss()(logits_x, targets_x)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs_u_w = torch.softmax(logits_u_w, dim=1)\n",
    "        max_probs, pseudo_label = torch.max(probs_u_w, dim=1)\n",
    "        mask = (max_probs >= T).float()\n",
    "\n",
    "    loss_u_all = nn.CrossEntropyLoss(reduction='none')(\n",
    "        logits_u_s, pseudo_label\n",
    "    )\n",
    "    loss_u = (loss_u_all * mask).mean()\n",
    "\n",
    "    return loss_x, LAMBDA_U * loss_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1546636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    labeled_loader: DataLoader,\n",
    "    unlabeled_loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float, float, int]:\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_loss_x = 0\n",
    "    total_loss_u = 0\n",
    "\n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    num_batches_epoch = len(unlabeled_loader)\n",
    "\n",
    "    for i, (batch_unlabeled) in enumerate(unlabeled_loader):\n",
    "        try:\n",
    "            (x_batch, targets_x_batch) = next(labeled_iter)\n",
    "        except StopIteration:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            (x_batch, targets_x_batch) = next(labeled_iter)\n",
    "\n",
    "        x_batch = x_batch.to(device)\n",
    "        targets_x_batch = targets_x_batch.to(device)\n",
    "\n",
    "        # O batch não rotulado é uma lista [weak_img, strong_img]\n",
    "        u_w_batch = batch_unlabeled[0][0].to(device)\n",
    "        u_s_batch = batch_unlabeled[0][1].to(device)\n",
    "\n",
    "        inputs = torch.cat((x_batch, u_w_batch, u_s_batch))\n",
    "        logits = model(inputs)\n",
    "\n",
    "        logits_x = logits[: x_batch.size(0)]\n",
    "        logits_u_w, logits_u_s = logits[x_batch.size(0) :].chunk(2)\n",
    "\n",
    "        loss_x, loss_u = fixmatch_loss(\n",
    "            logits_x, targets_x_batch, logits_u_w, logits_u_s\n",
    "        )\n",
    "        loss = loss_x + loss_u\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_loss_x += loss_x.item()\n",
    "        total_loss_u += loss_u.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches_epoch\n",
    "    avg_loss_x = total_loss_x / num_batches_epoch\n",
    "    avg_loss_u = total_loss_u / num_batches_epoch\n",
    "\n",
    "    return avg_loss, avg_loss_x, avg_loss_u\n",
    "\n",
    "\n",
    "def validate_model(\n",
    "    model: nn.Module, test_loader: DataLoader, device: torch.device\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss += criterion(outputs, labels).item() * labels.size(0)\n",
    "\n",
    "    loss /= total\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def train_model_with_fixmatch(\n",
    "    num_labeled_per_class: int, resnet_50: bool = False\n",
    ") -> float:\n",
    "    set_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    experiment_name = f\"FixMatch_{num_labeled_per_class}_labels_per_class\"\n",
    "\n",
    "    labeled_loader, unlabeled_loader, test_loader = get_dataloaders(\n",
    "        num_labeled_per_class\n",
    "    )\n",
    "\n",
    "    if resnet_50:\n",
    "        model = get_resnet50_for_cifar(NUM_CLASSES).to(device)\n",
    "        experiment_name += \"_ResNet50\"\n",
    "    else:\n",
    "        model = get_resnet18_for_cifar(NUM_CLASSES).to(device)\n",
    "\n",
    "    print(f\"Iniciando: {experiment_name}\")\n",
    "    print(f\"Usando device: {device}\")\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=LR,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    exp_dir = os.path.join(SAVE_DIR, experiment_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(exp_dir, 'training_logs.csv'), 'w') as f:\n",
    "        f.write(\"epoch,loss,loss_x,loss_u,test_acc,test_loss\\n\")\n",
    "\n",
    "    START_EPOCH = 1\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(os.path.join(exp_dir, 'best_model.pth'))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_acc = checkpoint['best_val_acc']\n",
    "        START_EPOCH = checkpoint['epoch'] + 1\n",
    "        print(f\"Carregado checkpoint do epoch {checkpoint['epoch']}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Nenhum checkpoint encontrado, iniciando treinamento do zero.\")\n",
    "\n",
    "    for epoch in range(START_EPOCH, TOTAL_EPOCHS + 1):\n",
    "\n",
    "        train_loss, train_loss_x, train_loss_u = train_one_epoch(\n",
    "            model, labeled_loader, unlabeled_loader, optimizer, device\n",
    "        )\n",
    "\n",
    "        test_acc, test_ce_loss = validate_model(model, test_loader, device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d}/{TOTAL_EPOCHS:03d} | \"\n",
    "            f\"Loss: {train_loss:.5f} (Lx: {train_loss_x:.5f}, Lu: {train_loss_u:.5f}) | \"\n",
    "            f\"Test Acc: {test_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "        with open(os.path.join(exp_dir, 'training_logs.csv'), 'a') as f:\n",
    "            f.write(\n",
    "                f\"{epoch},{train_loss:.5f},{train_loss_x:.5f},\"\n",
    "                f\"{train_loss_u:.5f},{test_acc:.2f},{test_ce_loss:.5f}\\n\"\n",
    "            )\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(\n",
    "                {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'best_val_acc': best_acc,\n",
    "                },\n",
    "                os.path.join(exp_dir, 'best_model.pth'),\n",
    "            )\n",
    "            print(f\"\\tModelo Salvo com acc: {best_acc:.2f}%\")\n",
    "\n",
    "        if epoch % SAVE_N_EPOCHS == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(exp_dir, f'model_epoch_{epoch}.pth'),\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Treinamento de {experiment_name} concluído. \"\n",
    "        f\"Melhor Acurácia: {best_acc:.2f}%\"\n",
    "    )\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    set_seed(SEED)\n",
    "    labeled_per_class_cases = [\n",
    "        1,  # Caso 1: 10 rótulos total\n",
    "        4,  # Caso 2: 40 rótulos total\n",
    "        25,  # Caso 3: 250 rótulos total\n",
    "        400,  # Caso 4: 4.000 rótulos total\n",
    "    ]\n",
    "\n",
    "    labeled_per_class_cases.append(250)\n",
    "\n",
    "    labeled_per_class_cases.sort()\n",
    "\n",
    "    results = dict()\n",
    "\n",
    "    for num_labeled in labeled_per_class_cases:\n",
    "        acc_resnet50 = train_model_with_fixmatch(num_labeled, resnet_50=True)\n",
    "        results[f\"{num_labeled} rótulos/classe (ResNet50)\"] = acc_resnet50\n",
    "\n",
    "        acc_resnet18 = train_model_with_fixmatch(num_labeled, resnet_50=False)\n",
    "        results[f\"{num_labeled} rótulos/classe (ResNet18)\"] = acc_resnet18\n",
    "\n",
    "    print()\n",
    "    print(\"#\" * 50)\n",
    "    print()\n",
    "    print(\"Resultados:\")\n",
    "    for case, acc in results.items():\n",
    "        print(f\"Caso {case}: Melhor Acurácia de Teste: {acc:.2f}%\")\n",
    "    print()\n",
    "    print(\"#\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf797b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fix-match-assignment-dl (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
